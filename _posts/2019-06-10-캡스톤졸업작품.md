---
title: 캡스톤 졸업작품-API를 활용한 인공지능 스피커
author: loveAlakazam
categories: [프로젝트2019]
tags: [인공지능 스피커, Keyword Detection, Seeed 2MIC Pi HAT ,GCP, bs4, Raspberry PI3, TTS, STT, Relay]
comments: false
---

<br>

> # 1. 소개 & 활용 기술 스택

<br>

> ## 소개

- "snow boy" 라고 부르면 "띵" 소리를 내며 반응을 합니다.
- 사용자의 명령어를 듣습니다. 
    - STT(Speech to Text) API로 인해서 사용자의 음성을 텍스트로 변환시킵니다.
    - 명령어에 대한 대답 기능 구현은 대답텍스트를 음성으로 변환하는 TTS(Text to Speech) API를 활용했습니다.
    - 응답해줄 수 있는 명령어가 명령어 리스트에 있습니다.
    - 명령어 리스트에 없는 명령어는 "죄송합니다. 다시 말씀해주세요" 라고 말한다.
- 사용자의 명령어에 따라서 전자제품(선풍기 & 전구) 을 제어(켜기/끄기)할 수 있습니다.


<br>

> ## 활용 기술 스택

- #### Language: `Python3`
- #### API : `GCP(STT, TTS)`
- #### Board: `RaspberryPI`
- #### Library: `Pixel ring`,`bs4(BeautifulSoup)`

<br>



<br><br>

> # 2. 구현과정

<br>

- ### Raspberry Pi 3 & Respeaker 2Mics Pi Hat을 이용하여 제어하기

<br>

- ### Keyword Detection API Snowboy를 활용하자.

<br>

- ### GCP(Google Cloud Platform)의 STT와 TTS API를 활용하자

<br>

- ### BeautifulSoup(bs4) API를 활용하여 네이버 날씨 사이트를 Web-Scraping/Crawling 하기

<br>

- ### Relay를 연결하여 선풍기와 스탠드를 제어해보자.

<br>

<br><br>

> # 3. 동작코드

<br>

- [소스코드 전체 url](https://github.com/loveAlakazam/Capstone2_prof.Mjung/tree/master/0523)

<br>

- #### 메인코드(updated_voice_main.py)

```python
# -*- encoding: utf-8 -*-
import snowboydecoder as sdecoder
import signal
import led_test as leds
import time
import sys
from textToSpeech import answer
from weatherforecast import parsing_weather_info,today_weather_status, today_weather_dust, tomorrow_weather_status 
import speech_listening
import os
import RPi.GPIO as GPIO

FAN_CHANNEL=12
LIGHT_CHANNEL=13

def turn_on(channel):
    GPIO.setmode(GPIO.BCM)
    GPIO.setup(channel, GPIO.OUT)
    GPIO.output(channel, GPIO.LOW)
    

def turn_off(channel):
    GPIO.setmode(GPIO.BCM)
    GPIO.setup(channel, GPIO.OUT)
    GPIO.output(channel, GPIO.HIGH)

interrupted=False

def signal_handler(signal, frame):
    global interrupted
    interrupted=True

def interrupt_callback():
    global interrupted
    return interrupted


if len(sys.argv)==1:
    print("Error: need to specify model name")
    print("Usage: python3 voice_reception_main.py model")
    sys.exit(-1)
   
model = sys.argv[1]
#simple mode(end/ saying hello / question weather)
cmdLists=[ 
            ['끝', '끝내자', '종료'],#0
            ['안녕', '안녕하세요', '하이'],#1
            ['날씨', '오늘 날씨', '오늘날씨 알려줘', '오늘 날씨 알려 줘'],#2
            ['미세먼지', '먼지', '미세먼지 알려 줘'],#3
            ['내일 날씨', '내일 날씨 알려줘'],#4
            ['불 켜','불켜줘', '불 켜 줘', '스탠드 켜 줘', '스탠드', '불','조명', '조명 켜줘', '불 켜주세요', '불켜주세요' ], #only light on
            ['불 꺼','불 꺼줘' '불 꺼 줘','불 꺼 줘','스탠드 꺼 줘', '조명 꺼줘', '불 꺼주세요', '불꺼주세요','불꺼줄래','불끌래','불끌게'], # only light off
            ['선풍기 켜줘', '더워', '덥다', '개더워', '너무 더워' ,'선풍기', '바람', '선풍기 바람', '선풍기바람', '선풍기 켜', '선풍기켜줘', '선풍기켜'], # only fan on
            ['선풍기 꺼줘','추워','춥다','개추워','너무 추워', '선풍기 꺼주세요', '선풍기 끌래', '선풍기 그만'], # only fan off
            ['기상 모드', '다 켜 줘','켜 줘', '켜 줘','둘다 켜줘','모두 켜줘','모두다 켜줘', '기상'], # fan and light on
            ['취침 모드', '다 꺼 줘','꺼 줘','둘다 꺼줘','모두 꺼줘','모두다 꺼줘', '취침']  # fan and light off
         ]

def main():
    while True:
        try:
            #1. key-word detection
            detector=sdecoder.HotwordDetector(model,sensitivity=0.6)
            print('Listening ... press Ctrl+c to exit..')
            detector.start(detected_callback=sdecoder.play_audio_file,
                           interrupt_check=interrupt_callback,
                           sleep_time=0.03)

            # if keyword detected keyword => using Speeh API
            # google Speech transcribe mic...
            # maximum listening time: about 65[s]
            #2. listening user's command
            leds.pixels.think()
            your_command=speech_listening.main()
            
            #3. check cmdLists
            #초기화. your_command가 cmdLists에 존재하지 않음을 의미함.
            check=-1 
            #your_command가 cmdLists에 있는지 확인한다.
            for cmd in cmdLists:
                if your_command in cmd: #cmd에 명령이 존재한다면
                    check=cmdLists.index(cmd) #cmd의 index추출
                    break #빠져나온다
             
            #명령어가 cmdList에 존재하지 않음.
            if check==-1:
                # led상태 : speak
                leds.pixels.speak()
                time.sleep(0.1)
                answer('죄송합니다. 다시 명령해주세요.')
                
            
            else: #명령어가 존재한다.
                if check==0: #명령어: 종료 -> 반응: led off & 띵소리만 낸다.
                    # led 상태: 불끈다.
                    turn_off(12)
                    turn_off(13)
                    sdecoder.play_audio_file()
                    #os.system('aplay terminated.wav')
                    leds.pixels.off()             

                elif check==1: #명령어: 인사 -> 반응: 인사
                    leds.pixels.speak()
                    answer('안녕하세요?')
                
                elif check==2: #명령어: 오늘날씨
                    leds.pixels.speak()
                    answer(today_weather_status())
                
                elif check==3: #명령어: 오늘 미세먼지
                    leds.pixels.speak()
                    answer(today_weather_dust())
                
                elif check==4: #명령어: 내일 날씨
                    leds.pixels.speak()
                    answer(tomorrow_weather_status())

                elif check==5:  # 조명on
                    turn_on(13)
                    time.sleep(1)
                    
                elif check==6:  # 조명off
                    turn_off(13)
                    time.sleep(1)
                    
                elif check==7:  # 선풍기on
                    turn_on(12)
                    time.sleep(1)
                    
                elif check==8:  # 선풍기off
                    turn_off(12)
                    time.sleep(1)
                    
                elif check==9:  # 조명&선풍기on
                    turn_on(13)
                    time.sleep(1)
                    turn_on(12)
                    time.sleep(1)
                    
                else:#check==10 조명&선풍기off
                    turn_off(13)
                    time.sleep(1)
                    turn_off(12)
                    time.sleep(1)
                   
            leds.pixels.off()
            time.sleep(0.1)
        except KeyboardInterrupt: #detected signals
            GPIO.cleanup()
            break
    
    leds.pixels.off()
    time.sleep(1)
    sys.exit(1)

if __name__=='__main__':
    main()
```

<br><br>

- #### 네이버 날씨 웹크롤러를 활용한 날씨정보 파싱(weatherforecast.py)


```python
# -*- coding: utf-8 -*-
"""
Created on Thu Apr  4 18:34:35 2019
@author: lovesAlakazam
"""
from bs4 import BeautifulSoup
from urllib.request import urlopen, Request

def parsing_weather_info(): # 오늘날씨 정보 웹에서 추출
    #가져올 페이지: 네이버 '오늘의 날씨' 검색 결과
    url='https://search.naver.com/search.naver?sm=tab_hty.top&where=nexearch&query=%EC%98%A4%EB%8A%98%EC%9D%98+%EB%82%A0%EC%94%A8'

    # 현재 url에 해당하는 페이지를 가져온다.
    page = urlopen(Request(url))

    # page를 읽는다.
    html = page.read()

    soup = BeautifulSoup(html, 'html5lib')
    #print(soup) 
    
    ## 오늘 날씨 정보를 알려줌.
    # 현재 위치한 지역
    loc_info=soup.find('span', class_='btn_select').find('em').text
    location= loc_info[:-1]
    return soup, location
    

def today_weather_status(): # 오늘 날씨 상태
    soup, location=parsing_weather_info()
    #기온:
    temperature=soup.find_all('span', class_='todaytemp')
    temperature=[x.text for x in temperature]
    
    #오늘 기온 temperature[0]
    today_weather_info='오늘 '+location+'의 기온은 '+temperature[0]+'도이고 '

    #오늘 날씨 상태: weather_info[2]
    cy=soup.find('p',class_='cast_txt').text
    now_status=cy[:cy.find(',')]
    today_weather_info +=(now_status+'입니다. ')
    
    #체감온도: weather_info[4]
    cy= soup.find('span',class_='sensible').text
    sensing_temp=cy[ cy.find(' ')+1:cy.find('˚')]
    today_weather_info+=('체감온도는 '+sensing_temp+ '도 입니다.')
    return today_weather_info


def today_weather_dust(): # 오늘 미세먼지 정보
    soup, location=parsing_weather_info()

    ## 미세먼지 상태 정보
    today_dust_info='오늘 '+location+'의 ' 
    cy = soup.find_all('dd', class_='lv2')  #리스트
    c=cy[0].text #미세먼지부분만 
    today_dust_info+=('미세먼지 상태는 '+c[c.find('㎥')+1:]+'입니다.')
    return today_dust_info


def tomorrow_weather_status(): # 내일 날씨 상태
    soup, location=parsing_weather_info()
    temperature=soup.find_all('span', class_='todaytemp')
    temperature=[x.text for x in temperature]
    tomorrow_weather_info='내일 '+location+'의 오전 기온은 '+temperature[1]+'도이고 오후는 '+temperature[2]+'도 입니다.'
    return tomorrow_weather_info
    
  

if __name__=='__main__':
    t1= today_weather_status()
    t2= today_weather_dust()
    t3= tomorrow_weather_status()
    print('{}\n{}\n{}\n'.format(t1,t2,t3))
```

<br><br>

- #### STT API 활용(speach_listening.py)

```python
#!/usr/bin/env python

# Copyright 2017 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Google Cloud Speech API sample application using the streaming API.
NOTE: This module requires the additional dependency `pyaudio`. To install
using pip:
    pip install pyaudio
Example usage:
    python transcribe_streaming_mic.py
"""

# [START speech_transcribe_streaming_mic]
from __future__ import division
import re
import sys

# ~/.local/lib/python3.5/site-packages/google/cloud/
from google.cloud import speech
from google.cloud.speech import enums
from google.cloud.speech import types
import pyaudio
from six.moves import queue

# Audio recording parameters
RATE = 16000
CHUNK = int(RATE / 10)  # 100ms


class MicrophoneStream(object):
    """Opens a recording stream as a generator yielding the audio chunks."""
    def __init__(self, rate, chunk):
        self._rate = rate
        self._chunk = chunk

        # Create a thread-safe buffer of audio data
        self._buff = queue.Queue()
        self.closed = True

    def __enter__(self):
        self._audio_interface = pyaudio.PyAudio()
        self._audio_stream = self._audio_interface.open(
            format=pyaudio.paInt16,
            # The API currently only supports 1-channel (mono) audio
            # https://goo.gl/z757pE
            channels=1, rate=self._rate,
            input=True, frames_per_buffer=self._chunk,
            # Run the audio stream asynchronously to fill the buffer object.
            # This is necessary so that the input device's buffer doesn't
            # overflow while the calling thread makes network requests, etc.
            stream_callback=self._fill_buffer,
        )

        self.closed = False

        return self

    def __exit__(self, type, value, traceback):
        self._audio_stream.stop_stream()
        self._audio_stream.close()
        self.closed = True
        # Signal the generator to terminate so that the client's
        # streaming_recognize method will not block the process termination.
        self._buff.put(None)
        self._audio_interface.terminate()

    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):
        """Continuously collect data from the audio stream, into the buffer."""
        self._buff.put(in_data)
        return None, pyaudio.paContinue

    def generator(self):
        while not self.closed:
            # Use a blocking get() to ensure there's at least one chunk of
            # data, and stop iteration if the chunk is None, indicating the
            # end of the audio stream.
            chunk = self._buff.get()
            if chunk is None:
                return
            data = [chunk]

            # Now consume whatever other data's still buffered.
            while True:
                try:
                    chunk = self._buff.get(block=False)
                    if chunk is None:
                        return
                    data.append(chunk)
                except queue.Empty:
                    break

            yield b''.join(data)


def listen_print_loop(responses):
    """Iterates through server responses and prints them.
    The responses passed is a generator that will block until a response
    is provided by the server.
    Each response may contain multiple results, and each result may contain
    multiple alternatives; for details, see https://goo.gl/tjCPAU.  Here we
    print only the transcription for the top alternative of the top result.
    In this case, responses are provided for interim results as well. If the
    response is an interim one, print a line feed at the end of it, to allow
    the next result to overwrite it, until the response is a final one. For the
    final one, print a newline to preserve the finalized transcription.
    """
    num_chars_printed = 0
    for response in responses:
        if not response.results:
            continue

        # The `results` list is consecutive. For streaming, we only care about
        # the first result being considered, since once it's `is_final`, it
        # moves on to considering the next utterance.
        result = response.results[0]
        if not result.alternatives:
            continue

        # Display the transcription of the top alternative.
        transcript = result.alternatives[0].transcript

        # Display interim results, but with a carriage return at the end of the
        # line, so subsequent lines will overwrite them.
        #
        # If the previous result was longer than this one, we need to print
        # some extra spaces to overwrite the previous result
        overwrite_chars = ' ' * (num_chars_printed - len(transcript))

        if not result.is_final:
            sys.stdout.write(transcript + overwrite_chars + '\r')
            sys.stdout.flush()

            num_chars_printed = len(transcript)

        else:
            your_command=transcript + overwrite_chars
            print(your_command)

            # Exit recognition if any of the transcribed phrases could be
            # one of our keywords.
            #if re.search(r'\b(exit|quit)\b', transcript, re.I):
            #    print('Exiting..')
            #    break

            num_chars_printed = 0
            return your_command


def main():
    # See http://g.co/cloud/speech/docs/languages
    # for a list of supported languages.
    language_code = 'ko-KR'  # a BCP-47 language tag

    # ~/.local/lib/python3.5/site-packages/google/cloud/
    client = speech.SpeechClient()
    config = types.RecognitionConfig(
        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=RATE,
        language_code=language_code)
    streaming_config = types.StreamingRecognitionConfig(
        config=config,
        interim_results=True)

    with MicrophoneStream(RATE, CHUNK) as stream:
        audio_generator = stream.generator()
        requests = (types.StreamingRecognizeRequest(audio_content=content)
                    for content in audio_generator)

        responses = client.streaming_recognize(streaming_config, requests)

        # Now, put the transcription responses to use.
        your_command=listen_print_loop(responses)
    
    return your_command


if __name__ == '__main__':
    main()
# [END speech_transcribe_streaming_mic]
```

<br><br>

- #### Respeaker 2Mics-Pi-hat 내장 LED (led_test.py)

```python
"""
LED light pattern like Google Home
"""

import apa102
import time
import threading
try:
    import queue as Queue
except ImportError:
    import Queue as Queue


class Pixels:
    PIXELS_N = 3

    def __init__(self):
        self.basis = [0] * 3 * self.PIXELS_N
        self.basis[3],self.basis[4]=2,2
        self.colors = [0] * 3 * self.PIXELS_N
        self.dev = apa102.APA102(num_led=self.PIXELS_N)

        self.next = threading.Event()
        self.queue = Queue.Queue()
        self.thread = threading.Thread(target=self._run)
        self.thread.daemon = True
        self.thread.start()

    def wakeup(self, direction=0):
        def f():
            self._wakeup(direction)

        self.next.set()
        self.queue.put(f)

    def listen(self):
        self.next.set()
        self.queue.put(self._listen)

    def think(self):# our project(voice_recept_main=> command listening)
        self.next.set()
        self.queue.put(self._think)

    def speak(self):
        self.next.set()
        self.queue.put(self._speak)

    def off(self):
        self.next.set()
        self.queue.put(self._off)

    def _run(self):
        while True:
            func = self.queue.get()
            func()

    def _wakeup(self, direction=0):
        self.colors=[0,0,0,48,48,0,0,0,0]
        for i in range(1, 25):
            colors = [i * v for v in self.basis]
            self.write(colors)
            time.sleep(0.01)

        self.colors = colors

    def _listen(self):
        
        for i in range(1, 25):
            colors = [i * v for v in self.basis]
            self.write(colors)
            time.sleep(0.01)

        self.colors = colors

    def _think(self):
        colors = self.colors

        self.next.clear()
        while not self.next.is_set():
            colors = colors[3:] + colors[:3]
            self.write(colors)
            time.sleep(0.2)

        t = 0.1
        for i in range(0, 5):
            colors = colors[3:] + colors[:3]
            self.write([(v * (4 - i) / 4) for v in colors])
            time.sleep(t)
            t /= 2

        # time.sleep(0.5)

        self.colors = colors

    def _speak(self):
        self.colors=[48,0,48,48,0,48,48,0,48]
        colors = self.colors
        gradient = -1
        position = 24

        self.next.clear()
        while not self.next.is_set():
            position += gradient
            self.write([(v * position / 24) for v in colors])

            if position == 24 or position == 4:
                gradient = -gradient
                time.sleep(0.2)
            else:
                time.sleep(0.01)

        while position > 0:
            position -= 1
            self.write([(v * position / 24) for v in colors])
            time.sleep(0.01)

        # self._off()

    def _off(self):
        self.write([0] * 3 * self.PIXELS_N)

    def write(self, colors):
        for i in range(self.PIXELS_N):
            self.dev.set_pixel(i, int(colors[3*i]), int(colors[3*i + 1]), int(colors[3*i + 2]))

        self.dev.show()


pixels = Pixels()


if __name__ == '__main__':
    while True:

        try:
            print('wake up')
            pixels.wakeup()
            time.sleep(3)
            

            print('listening...')
            pixels.listen()
            time.sleep(3)
            

            print('thinking...')
            pixels.think()
            time.sleep(3)
            

            print('speaking...')
            pixels.speak()
            time.sleep(3)
            
            print('end...')
            pixels.off()
            time.sleep(3)
        except KeyboardInterrupt:
            break


    pixels.off()
    time.sleep(1)
```

<br><br>

- #### TTS API 활용(textToSpeech.py)

```python
#!/usr/bin/env python

# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Google Cloud Text-To-Speech API sample application .
Example usage:
    python quickstart.py
"""

from google.cloud import texttospeech
import os
def answer(txt):
    # [START tts_quickstart]
    """Synthesizes speech from the input string of text or ssml.
        Note: ssml must be well-formed according to:
        https://www.w3.org/TR/speech-synthesis/
    """
    # Instantiates a client
    client = texttospeech.TextToSpeechClient()

    # Set the text input to be synthesized
    synthesis_input = texttospeech.types.SynthesisInput(text=txt)

    voice = texttospeech.types.VoiceSelectionParams(
    language_code='ko-KR',  #Korean: ko-KR , 'en-US'
    ssml_gender=texttospeech.enums.SsmlVoiceGender.MALE)

    # Select the type of audio file you want returned
    audio_config = texttospeech.types.AudioConfig(
    audio_encoding=texttospeech.enums.AudioEncoding.MP3)

    # Perform the text-to-speech request on the text input with the selected
    # voice parameters and audio file type
    response = client.synthesize_speech(synthesis_input, voice, audio_config)

    # The response's audio_content is binary.
    with open('output.mp3', 'wb') as out:
        # Write the response to the output file.
        out.write(response.audio_content)    
    os.system('mpg123 output.mp3')
    # [END tts_quickstart]

if __name__ == '__main__':
    txt='컴 공 최은강 취뽀하자'
    answer(txt)#playing tts with txt
```


<br><br><br>

> # 4. 시연영상

<b> 시연 영상은 제 네이버 블로그에 수록되어있어서 아래 링크를 참고하시면 됩니다.
</b>


- [2019.03.23 - Raspberry Pi3 & Respeaker 2Mics Pi Hat 제어](https://blog.naver.com/rose1216_/221495561068)

- [2019.03.24 - keyword Detection Snowboy API ](https://blog.naver.com/rose1216_/221495778164)

- [2019.03.31 - GCP STT(Speech to Text)  API 시연영상](https://blog.naver.com/rose1216_/221501622704)


- [2020.04.04 - bs4를 이용하여 네이버 날씨에서 날씨관련 데이터를 수집하기](https://blog.naver.com/rose1216_/221505000508)


- [2020.04.05- GCP TTS(Text to Speech)](https://blog.naver.com/rose1216_/221505878423)

- [GCP TTS 사용방법](https://blog.naver.com/rose1216_/221751702177)


- [2020.05.20 - 릴레이모듈을 이용하여 전자제품과 연결](https://blog.naver.com/rose1216_/221542450240)


<br><br><br>

# 5. 느낀점
